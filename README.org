* Abstract

hunchentoot-recycling-taskmaster is a taskmaster implementation for Hunchentoot,
aiming to improve connection establishment efficiency through thread-pooling
and flexible thread count adjustment.

* Performance tl;dr

[[file:pics/benchmark-result-2025-12-26/sleep_1ms/keep-alive/100_connections_requests_sec__sleep_1ms__keep-alive__higher_is_better.svg]]

[[file:pics/benchmark-result-2025-12-26/sleep_1ms/no-keep-alive/100_connections_requests_sec__sleep_1ms__no_keep-alive__higher_is_better.svg]]

On this benchmark, my HTTP handler *always responds after 1ms* to simulate some workloads.

- Hunchentoot is an all-rounder. It works well on Keep-alive connections. I think it is good for typical use-cases.
- If your workload does not utilize Keep-alive, hunchentoot-recycling-taskmaster may be useful.
- Woo is very difficult to use. Woo seems fast only when your handlers work with strictly no latency. See [[#about-woo][About Woo]] below.

In detail, See [[#benchmark][Benchmark]] below.
  
* How to use

** Caution about Lispworks

On Lispworks, hunchentoot-recycling-taskmaster does not works because
Hunchentoot on that does not handle a listen socket directly.

** Installation

*** Loading

Currently hunchentoot-recycling-taskmaster is not on Quicklisp, Ultralisp or ocicl.
Please download it by yourself.

#+begin_src sh
  cd ~/quicklisp/local-projects
  git clone https://github.com/y2q-actionman/hunchentoot-recycling-taskmaster.git
#+end_src

#+begin_src lisp
  (ql:register-local-projects)            ; Do if required
  (ql:quickload "hunchentoot-recycling-taskmaster")
#+end_src

*** Running tests

#+begin_src common-lisp
  (ql:quickload "hunchentoot-recycling-taskmaster-test")
  (asdf:test-system '#:hunchentoot-recycling-taskmaster)
#+end_src

** Starting/stopping server

You can use hunchentoot-recycling-taskmaster just by changing
=hunchentoot:acceptor= to =hunchentoot-recycling-taskmaster:parallel-acceptor=, or
=hunchentoot:easy-acceptor= to =hunchentoot-recycling-taskmaster:parallel-easy-acceptor=.

#+begin_src common-lisp
  (defparameter *test-server*
    (make-instance 'hunchentoot-recycling-taskmaster:parallel-easy-acceptor
		   :port 4242))
  (hunchentoot:start *test-server*)
#+end_src

#+begin_src bash
  curl "http://127.0.0.1:4242/yo"
  # => "Hey!"
#+end_src

To stop it, =hunchentoot:stop= can be used.

#+begin_src common-lisp
  (hunchentoot:stop *test-server*)
#+end_src

See [[file:demo.lisp][demo.lisp]] for the sample codes above.

** API

These symbols are exported from =hunchentoot-recycling-taskmaster= package.
Please see their docstring also.

- [Class] =parallel-acceptor=
- [Class] =parallel-easy-acceptor=
- [Class] =parallel-ssl-acceptor=
- [Class] =parallel-easy-ssl-acceptor=
- [Class] =recycling-taskmaster=
- [Variable] =*default-standby-thread-count*=
  
- [Function] =abandon-acceptor=
- [Condition] =recycling-taskmaster-corrupted-error=

* How it works

hunchentoot-recycling-taskmaster is a hunchentoot taskmaster
implementation that aims to improve the efficiency of connection
establishment. This section shows how it works.

** Hunchentoot; make one thread per connection.

[[file:pics/architecture/hunchentoot-architecture.dot.png]]

Hunchentoot uses one thread per one connection.  It means when a
client use one keep-alive connection for multiple requests,
Hunchentoot dedicate one thread to that connection.  There is some
delay for new connections, but it works well on keep-alive
connections.

** quux-hunchentoot and cl-tbnl-gserver-tmgr; Thread pooling

[[file:pics/architecture/hunchentoot-thread-pooling.dot.png]]

These implementations utilize a thread pool around Hunchentoot.
Instead of making a new thread for a new connection, they reuse
threads kept in its thread pool, reducing latency for new connections.

However, their benchmarks don't show significant differences from the
original Hunchentoot. I suspect this is for two reasons:

1. HTTP benchmarking tool, such as "wrk", utilizes keep-alive
   connections.  Effects of thread-pooling are limited at the
   beginning of benchmarking.
2. Their thread-pool's size may be fixed. They cannot
   increase threads like Hunchentoot even on high workloads.

** hunchentoot-recycling-taskmaster

[[file:pics/architecture/hunchentoot-recycling-taskmaster-architecture.dot.png]]

hunchentoot-recycling-taskmaster tries to take both, thread-pooling
and changing the number of threads dynamically. In
hunchentoot-recycling-taskmaster, threads don't only work on connected
sockets but also some management tasks, such as accepting a new
connection from the listen socket, creating a new thread, or
terminating themselves.

For this management, all threads share the listen socket for
synchronizing acceptance and track how many threads on it.  Since
using the listen socket, hunchentoot-recycling-taskmaster successfully
implements these mechanisms without adding new dependencies to
hunchentoot.

* Benchmark

** Running benchmarks
To run benchmark by yourself, do below:

#+begin_src common-lisp
  (ql:quickload "hunchentoot-recycling-taskmaster-benchmark")
  (asdf:test-system '#:hunchentoot-recycling-taskmaster-benchmark)
#+end_src

** My Environments

*** My machine

| cl function name            |                                     |
|-----------------------------+-------------------------------------|
| LISP-IMPLEMENTATION-TYPE    | SBCL                                |
| LISP-IMPLEMENTATION-VERSION | 2.2.9.debian                        |
| MACHINE-TYPE                | X86-64                              |
| MACHINE-VERSION             | 13th Gen Intel(R) Core(TM) i7-1360P |
| SOFTWARE-TYPE               | Linux                               |
| SOFTWARE-VERSION            | 5.15.153.1-microsoft-standard-WSL2  |

*** Server library versions and parameters

| name                             | parameters             | version | Git commit                               |
|----------------------------------+------------------------+---------+------------------------------------------|
| hunchentoot-recycling-taskmaster | standby-thread-count 8 |   0.0.1 | 82913edaf3f65afb189f0d72ddeb6339bf0499ae |
| hunchentoot                      |                        |   1.3.1 | d1617e9d4eab6cb801c56cf36d9b0aab134fb7e6 |
| quux-hunchentoot                 |                        |   1.0.2 | quux-hunchentoot-20211230-git            |
| cl-tbnl-gserver-tmgr             | max-thread-count 8     |   0.1.1 | 1ae71c9324e876761cd1ee51768a34f0793e6879 |
| wookie                           |                        |  0.3.15 | 1f74b6c24b463c1e6fff35377e477934f72bac20 |
| woo                              | worker-num 8           |  0.12.0 | 7f5219c55d49190f5ae17b123a8729b31c5d706e |

*** benchmarking tool

I used *wrk*, like below

#+begin_src bash
  # keep-alive
  wrk -t 4 -c 100 -d 10 http://localhost:4242/yo
  # simulating no keep-alive
  wrk "-H Connection: close" -t 4 -c 100 -d 10 http://localhost:4242/yo
#+end_src

** Results

*** keep-alive, sleep 1ms

[[file:pics/benchmark-result-2025-12-26/sleep_1ms/keep-alive/100_connections_requests_sec__sleep_1ms__keep-alive__higher_is_better.svg]]
[[file:pics/benchmark-result-2025-12-26/sleep_1ms/keep-alive/100_connections_latency(us)__sleep_1ms__keep-alive__lower_is_better.svg]]

On this benchmark, my HTTP handler *always responds after 1ms* to simulate some workloads.

Hunchentoot is quite fast.  As mentioned above, Hunchentoot assigns
one thread per connection and keeps using that thread until the
connection is closed.  Therefore, if a connection is kept alive,
there's no delay from thread creation, and it doesn't become
Hunchentoot slow.  =wrk= uses keep-alive by default, so this result
benefits from this.

cl-tbnl-gserver-tmgr is not very fast. This is presumably because it
only assigns 8 threads as workers.

Woo is not fast even set to 8 threads. See [[#about-woo][About Woo]] below.

*** no keep-alive, sleep 1ms

[[file:pics/benchmark-result-2025-12-26/sleep_1ms/no-keep-alive/100_connections_requests_sec__sleep_1ms__no_keep-alive__higher_is_better.svg]]
[[file:pics/benchmark-result-2025-12-26/sleep_1ms/no-keep-alive/100_connections_latency(us)__sleep_1ms__no_keep-alive__lower_is_better.svg]]

On this benchmark, HTTP handler still causes 1ms latency, and I
simulated "no keep-alive" by adding =wrk= option ="-H Connection:
close"= .

Hunchentoot became quite slow. In this test, Hunchentoot works like
"one thread per one request" so latencies caused by creating a new
thread affected the result.

hunchentoot-recycling-taskmaster is designed to work well in this
situation.

*** keep-alive, sleep 0ms

[[file:pics/benchmark-result-2025-12-26/sleep_0ms/keep-alive/100_connections_requests_sec__sleep_0ms__keep-alive__higher_is_better.svg]]
[[file:pics/benchmark-result-2025-12-26/sleep_0ms/keep-alive/100_connections_latency(us)__sleep_0ms__keep-alive__lower_is_better.svg]]

On this benchmark, I set HTTP handler's latency to 0ms. (However some
latencies caused by small computations still exist. See [[#about-woo][About Woo]] below.)

hunchentoot-recycling-taskmaster became fast. I think this is because
threads are accepting connections in parallel.

*** no keep-alive, sleep 0ms

[[file:pics/benchmark-result-2025-12-26/sleep_0ms/no-keep-alive/100_connections_requests_sec__sleep_0ms__no_keep-alive__higher_is_better.svg]]
[[file:pics/benchmark-result-2025-12-26/sleep_0ms/no-keep-alive/100_connections_latency(us)__sleep_0ms__no_keep-alive__lower_is_better.svg]]

On this benchmark, I set HTTP handler's latency to 0ms like above, and set
=wrk= option ="-H Connection:close"= .

Here, Woo finally takes first place, with
hunchentoot-recycling-taskmaster coming in a close second.

*** Other results

- See [[pics/benchmark-result-2025-12-26/benchmark-result-2025-12-26.org][benchmark-result-2025-12-26.org]] for graphs with "400 connections" parameters
- See [[benchmark/benchmark-result-2025-12-26][this directory]] for raw data.

** About Woo

*** How to sleep?

Woo becomes disastrously slow if the handler is even slightly delayed.
With the above "1ms sleep" benchmark, Woo starts up as follows, which
leads to terrible results:

#+begin_src lisp
  (defparameter *handler-sleep-seconds* 0)

  (defun handler-small-sleep ()
    (sleep *handler-sleep-seconds*))

  (woo:run
   (lambda (env)
     (declare (ignore env))
     (handler-small-sleep)
     '(200 (:content-type "text/plain") ("Hello, World")))
   :worker-num 8)
#+end_src

This is because Woo is an async server that handles multiple
connections simultaneously in one thread.  If a delay occurs in the
processing of one connection, the processing of all other connections
in the same thread will be delayed.  Given its async architecture,
=sleep= like above is obviously discouraged.

In cases like this, you generally don't sleep inside the async
server's event loop.  You run time-consuming processing outside the
event loop, and when it's finished, you notify the event loop of the
content to be sent and received, or set up a callback to be called.

But for some reason, Woo doesn't seem to have such a mechanism.
I couldn't find it.
# another one says "Woo has no async foundation", too: https://news.ycombinator.com/item?id=29019217
# https://lisp-journey.gitlab.io/blog/why-turtl-switched-from-lisp-to-js/
# https://killtheradio.net/technology/cl-async-non-blocking-asynchronous-programming-for-common-lisp/

[[https://github.com/quickdocs/quickdocs-api/tree/master][quickdocs-api]], which is said to use Woo, and [[https://github.com/TechEmpower/FrameworkBenchmarks/pull/8313][this code]]
do not seem to take such considerations into account.
# (It may be a one reason of not good benchmark linked by [[https://www.reddit.com/r/Common_Lisp/comments/1506kqb/comment/js220a6/][this Reddit thread]].)

Some people on [[https://news.ycombinator.com/item?id=29019217][here]] or [[https://www.reddit.com/r/Common_Lisp/comments/1hmxv4k/comment/m4feb4b/][here]] have said that "offloading is possible
with lparallel", but I have yet to find any code that actually does
this. The following code, which creates a thread in naive, will result
in an error.

#+begin_src common-lisp
  ;; Making a thread like below don't works because `woo.ev:*evloop*' is NIL.
  (defparameter *woo-callback-threads-app*
    (lambda (_env)
      (declare (ignore _env))
      (lambda (callback)
	(bt:make-thread (lambda ()
			  (funcall callback '(200 (:content-type "text/plain") ("Hello, World"))))))))

  (clack:clackup *woo-callback-threads-app* :server :woo)

  ;; Binding like that does not works also.
  (let ((evloop woo.ev:*evloop*))
    (bt:make-thread (lambda (&aux (woo.ev:*evloop* evloop))
		      (funcall callback '(200 (:content-type "text/plain") ("Hello, World"))))))
#+end_src

This is because the =woo.ev:*evloop*= variable, which is the
actual event loop, cannot be referenced in the thread created with
=(make-thread)= and will become =NIL=.

# By the way, using lparallel like above will be "one-thread-per-request" model.
# I think that will poorly perform than "one-thread-per-connection" of Hunchentoot.

This problem can be solved by directly handling libev, which Woo
depends on, directly handling the event loop, and creating sleep etc.
[[benchmark/about_woo/woo-timer-cb--written-by-gemini--working.lisp][This code]] was written by Gemini-CLI.  It certainly works, and the
benchmarks aren't bad. However, I don't know how this code works.
Don't ask me.

So, to benchmark "sleep 1ms" in Woo, I have no choice but to use the
terrible code example above.  This is a huge disadvantage to Woo, but
it's unavoidable due to the lack of support in Woo and the way
quickdocs-api is written.

*** Don't evaluate special variables

Some people might look at the above [[#keep-alive-sleep-0ms][keep-alive sleep 0ms result]] and
wonder "Maybe Woo isn't as fast as people say?"  I thought the same
thing, investigated and found that the cause was that the handler
definition for this benchmark calls =(handler-small-sleep)=
defined above.

#+begin_src common-lisp
  (defparameter *handler-sleep-seconds* 0)

  (defun handler-small-sleep ()
    (sleep *handler-sleep-seconds*))
#+end_src

This code does the following:
1. Eval the special variable (which becomes 0).
2. Call =cl:sleep= with 0 (which should return immediately).

These two processes don't take much time, but unfortunately they cause
problems in woo.  Changing =handler-small-sleep= as follows made it
faster, as shown in the following graph.

#+begin_src common-lisp
  (defun handler-small-sleep ()
    )
#+end_src

[[file:pics/benchmark-result-2025-12-26/sleep_0ms_no_special_vars/keep-alive/100_connections_requests_sec__sleep_commented_out__keep-alive__higher_is_better.svg]]
[[file:pics/benchmark-result-2025-12-26/sleep_0ms_no_special_vars/keep-alive/100_connections_latency(us)__sleep_commented_out__keep-alive__lower_is_better.svg]]

So, to get the most out of Woo's performance, you should not evalutate
special variables either.  In other words, in Woo your handlers must
work with strictly no latency.

If Woo's situation doesn't change, it could be said that Woo is very
fast only when your handlers don't compute anything.  I think Woo is
*only* good for serving static pages
# or microbenchmarking.
# but why did such a thing by Lisp? Use nginx.

* TODO TODO list

** register to systems

- [ ] quicklisp
- [X] ultralisp
- [ ] ocicl

** Benchmark other servers

- [ ] conserv :: Causing =cl:type-error= when I used its sample code.
- [ ] house :: Very fragile. See [[benchmark/about_house/about_house.org][my memo]] .
- [ ] teepeedee2 :: cannot be loaded by my machine because of [[benchmark/loading.org]["heap exhausted" error]].

** Ideas

- [ ] Using atomic variables -- its impact is small on SBCL, large on Allegro CL. See [[/atomic-op-taskmaster]] in this repository.
- [ ] make the number of =standby-thread-count= to variadic.

* License

BSD 2-Clause, same as Hunchentoot. See [[file:LICENSE][LICENSE]].
